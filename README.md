# SpeechTasks
This is a list of speech tasks and datasets, which can provide training data for Generative AI, AIGC, AI model training, intelligent speech tool development, and speech applications.

## Tasks

|Task|DataSets|Input Mode|Output Mode|Modeling Target|Level|Description|
| :-----|:----- |:----- |:----- |:----- |:----- |:----- |
|Accent Classification|[AccentDB Extended Dataset](https://accentdb.org)|Audio|Label|Classification|Accent|Accent classification involves the recognition and classification of specific speech accents.The task involves the recognition and classification of specific speech accents. The possible answers include American, Australian, Bangla, British, Indian, Malayalam, Odiya, Telugu, or Welsh. The objective is to correctly identify these accents based on the given speech samples, contributing to a system's ability to understand and interact with various speakers.|
|Dialogue Act Classification|[DailyTalk Dataset](https://github.com/keonlee9420/DailyTalk)|Audio|Label|Classification|Understanding|Dialogue act classification aims to identify the primary purpose or function of an utterance within its dialogue context.The aim of this task is to identify the action in the audio. The possible answers could be *question*, *inform*, *directive*, or *commissive*. These identification tasks are important, as dialogue acts are central to understanding human conversation and dialogue-based AI system communication.|
|Dialogue Act Pairing |[DailyTalk Dataset](https://github.com/keonlee9420/DailyTalk)|Audio, Label|Binary Label|Binary Classification|Understanding|Dialogue act pairing involves assessing the congruence of dialogue acts—that is, whether a response dialogue act is appropriate given a query dialogue act. The objective is to determine whether a given dialogue act pairing is congruent or not. The answer could either be true or false. Being able to accurately judge the appropriateness of dialogue acts is key for a universal speech model to understand and participate in human conversations effectively.|
|Dialogue Emotion Classification |[DailyTalk Dataset](https://github.com/keonlee9420/DailyTalk)|Audio|Label|Classification|Emotion|Dialogue emotion classification is a task that assesses an AI model's ability to identify the most suitable emotion in a given dialogue extract. The main goal of this task is to correctly identify the communicated emotion in an audio clip. Possible answers include anger, disgust, fear, sadness, happiness, surprise, or no emotion. It is an evaluation of the model's capacity to interpret and distinguish emotions conveyed through speech, accounting both for linguistic content and paralinguistic indicators.|
|Emotion Recognition |[Multimodal EmotionLines Dataset](https://affective-meld.github.io/)|Audio|Label|Classification|Emotion|Emotion recognition aims to identify the most appropriate emotional category for a given utterance.Recognizing the emotion expressed in an utterance can be quite challenging. While we can sometimes identify emotion from the linguistic content alone, the more important factors often lie in paralinguistic features — like pitch, rhythm, and other prosodic elements. For a universal speech model, understanding these paralinguistic features is crucial, as they distinguish speech from mere text in a significant manner.|
|Enhancement Detection |[LibriTTS-TestClean](https://huggingface.co/datasets/DynamicSuperb/EnhancementDetection_LibriTTS-TestClean_WHAM)|Audio|Binary Label|Binary Classification|Acoustic|Enhancement detection is a task focused on determining whether a given audio has been created or modified by a speech enhancement model. The objective of enhancement detection is to ascertain if an audio file has been created or altered by a speech enhancement model. The expected answer is either yes or no. The task poses a challenging problem because the speech model must not only process the content of the speech but also detect minute modifications that might indicate enhancement. |
|HowFarAreYou  |[3DSpeaker Dataset](https://github.com/alibaba-damo-academy/3D-Speaker)|Audio|Scalar|Regression|Acoustic|The HowFarAreYou task aims to determine the distance of the speaker from the source of sound. The task's goal is to ascertain the approximate distance of a speaker, based on the provided audio or speech. The task's response could be an exact value, such as 0.4m, 2.0m, or 4.0m, indicating the speaker's distance from the sound source. Gauging the speaker's distance provides insights into the audio's spatial characteristics, which forms a crucial aspect of auditory scene analysis. |
|Intent Classification  |[FluentSpeechCommands Dataset](https://fluent.ai/fluent-speech-commands-a-dataset-for-spoken-language-understanding-research/)|Audio|Sequence Label|Classification|Understanding|Intent classification aims to identify the actionable item behind a spoken message. The objective of this task is to understand and categorize the intent performed by a spoken message. The recognized actions can vary, including activate, bring, change language, deactivate, decrease, or increase. Identifying the intent accurately is pivotal for building reliable speech-based applications and interfaces. We categorize this task into three types: Action, Location, and Object.|
|Language Identification |[VoxForge Dataset](https://www.voxforge.org/)|Audio|Label|Classification|Language|Language Identification task is aimed to determine the language spoken in a given speech recording. The main goal of this task is to identify the language spoken in a specific speech recording. This is an essential part of speech processing, as it facilitates the understanding and translations for different languages. The language spoken could be German, English, Spanish, Italian, Russian, or French.|
|MultiSpeaker Detection |[LibriSpeech-TestClean Dataset](http://www.openslr.org/12/)<br />[VCTK Dataset](https://datashare.ed.ac.uk/handle/10283/3443)|Audio|Binary Label|Binary Classification|Speaker|MultiSpeaker Detection aims to analyze the speech audio to determine whether there is more than one speaker present in it. The core objective of this task is to analyze the speech audio for the presence of more than one speaker. It is crucial for a universal speech model to detect this as the presence of multiple speakers can alter the context and understanding of the spoken content.|
|Noise Detection  |[LJSpeech dataset](https://keithito.com/LJ-Speech-Dataset/)<br />[VCTK Dataset](https://datashare.ed.ac.uk/handle/10283/3443)<br />[Musan Dataset](https://www.openslr.org/17/)|Audio|Binary Label|Binary Classification|Acoustic|Noise Detection aims to idenetify if the speech audio is clean or mixed with noises.The objective of noise detection is to ascertain if an audio file has been added the noise. 
The expected answer is either yes or no. 
There are many types of noises - like music, speech, gaussian or others.
The task poses a challenging problem because the speech model must not only process the content of the speech but also understand the degradation of speech. |
|Noise SNR Level Prediction |[VCTK Dataset](https://datashare.ed.ac.uk/handle/10283/3443)<br />[Musan Dataset](https://www.openslr.org/17/)|Audio|Scalar|Regression|Acoustic|Noise SNR Level Prediction aims to predict the signal-to-noise ratio of the speech audio.The objective of noise SNR level prediction is to evaluate the noise SNR level of an audio file. 
The expected answer could be zero, five, ten, fifteen or zero. 
There are many types of noises - like music, speech, gaussian or others.
The task poses a challenging problem because the speech model must not only process the content of the speech but also understand the degree of noise degradation. |
|Reverberation Detection |[LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)<br />[VCTK Dataset](https://datashare.ed.ac.uk/handle/10283/3443)<br />[RIRs Noises Dataset](https://www.openslr.org/28/)|Audio|Binary Label|Binary Classification|Acoustic|Reverberation Detection aims to detect if the speech audio is clean or mixed with room impulse responses (RIRs) and noises, that is to say reverberation noises. The objective of reverberation detection is to ascertain if an audio file has been added the reverberation noises. 
The expected answer is either clean or noisy. 
The reverberation noises can be originated from large room, medium room or small room. 
The task poses a challenging problem because the speech model must not only process the content of the speech but also understand the degradation of speech in reververation cases. |
|Sarcasm Detection |[MUStARD Dataset](https://github.com/soujanyaporia/MUStARD)|Audio|Binary Label|Binary Classification|Understanding|Sarcasm Detection aims to detect if the sarcasm or the irony present in the speech audio. The objective of sarcasm detection is to recognize the presence of sarcasm or ironic expressions in the speech. 
The expected answer is either true or false. 
The task poses a challenging problem because the speech model should understand upper level of the semantic information. |
|Speaker Counting |[MUStARD Dataset](https://github.com/soujanyaporia/MUStARD)|Audio|Label|Classification|Speaker|Speaker Counting aims to identify the total number of speaker in speech audio. The objective of speaker counting is to determine the number of speakers in the audio recording. 
The expected answer should be one, two, three, four, or five.
The task poses a challenging problem because the speech model should undersdand the pattern of differnet speakers. |
|Speaker Verification |[LibriSpeech-TestClean Dataset](https://www.openslr.org/12)<br />[VCTK Dataset](https://datashare.ed.ac.uk/handle/10283/3443)
|Audio, Audio|Binary Label|Binary Classification|Speaker|Speaker verification aims to verify whether the two given speech audios are from the same speaker. The objective of speaker verification is to exam if the patterns in the two audio recordings come from the same speaker. 
The expected answer is either yes or no.
The task poses a challenging problem because the speech model should undersdand the pattern of differnet speakers. |
|Speech Command Recognition |[Google Speech Commands V1 Dataset](https://huggingface.co/datasets/speech_commands)
|Audio|Label|Classification|Content|Speech Command Recognition aims to identify the spoken command. The objective of speech command recognition is to comprehend and grasp the command presented in the speech. 
The expected answer should be yes, no, up, down, left, right, on, off, stop, go, zero, one, two, three, four, five, six, seven, eight, nine, bed, bird, cat, dog, happy, house, marvin, sheila, tree, wow, or silence.
The task poses a challenging problem because the speech model should understand the content information from the speech audios. |
|Speech Detection |[LJSpeech dataset](https://keithito.com/LJ-Speech-Dataset/)<br />[LibriSpeech-TestClean Dataset](https://www.openslr.org/12)<br />[LibriSpeech-TestOther Dataset](https://www.openslr.org/12)|Audio|Binary Label|Binary Classification|Acoustic|Speech Detection aims to identify whether the given audio clip contains real speech or not. The objective of speech detection is to analyze the audio and determine whether it consists of real speech or not. 
The expected answer is either yes or no.
The task poses a challenging problem because the speech model should understand not only the content information from the speech audios but the pattern of human voice. |
|Speech Text Matching |[LJSpeech dataset](https://keithito.com/LJ-Speech-Dataset/)<br />[LibriSpeech-TestClean Dataset](https://www.openslr.org/12)<br />[LibriSpeech-TestOther Dataset](https://www.openslr.org/12)|Audio, Text|Binary Label|Binary Classification|Content|Speech Text Matching aims to determine if the speech and text are matched. The objective of speech text matching is to assess whether the speech and text share the same underlying message or not. 
The expected answer is either yes or no.
The task poses a challenging problem because the speech model should understand the content information from the speech audios. |
|Spoken Term Detection|[LJSpeech dataset](https://keithito.com/LJ-Speech-Dataset/)<br />[LibriSpeech-TestClean Dataset](https://www.openslr.org/12)<br />[LibriSpeech-TestOther Dataset](https://www.openslr.org/12)|Audio, Text|Binary Label|Binary Classification|Content|Spoken Term Detection aims to check for the existence of the given word in the speech. The objective of spoken term detection is to analyze the speech and indicate whether the word is mentioned or not. 
The expected answer is either yes or no.
The task poses a challenging problem because the speech model should understand the content information from the speech audios. |
|Spoof Detection|[ASVspoof 2015 Dataset](https://datashare.ed.ac.uk/handle/10283/853)<br />[ASVspoof 2017 Datase](https://datashare.ed.ac.uk/handle/10283/3055)|Audio|Binary Label|Binary Classification|Acoustic|Spoof Detection aims to classify whether the given utterance is a spoofed voice or an authentic recording. The objective of spoof detection is to verify if the provided speech is a result of voice manipulation for spoofing purposes. 
The expected answer is either spoofed or authentic.
For a universal speech model, understanding these paralinguistic features is crucial, as they distinguish speech from mere text in a significant manner.|
|Stress Detection|[MIR-SD Dataset](http://mirlab.org/dataset/public/)|Audio|Binary Label|Binary Classification|Acoustic|Stress Detection aims to determine the stress placement in English vocabulary. The objective of stress detection is to analyze the stress patterns in English words. 
The expected answer should be zero, one, two, three, four, or five.
For a universal speech model, understanding these paralinguistic features is crucial, as they distinguish speech from mere text in a significant manner.|
